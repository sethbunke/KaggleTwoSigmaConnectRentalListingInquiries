{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adjust the old x pushing it in the direction of gradx with the force learning_rate. Subtracting learning_rate * gradx. Remember the gradient is initially in the direction of steepest ascent so subtracting learning_rate * gradx from x turns it into steepest descent. You can make sure of this yourself by replacing the subtraction with an addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent_update(x, gradx, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs a gradient descent update.\n",
    "    \"\"\"\n",
    "    # TODO: Implement gradient descent.\n",
    "    \n",
    "    # Return the new value for x\n",
    "    return x - (learning_rate * gradx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0: Cost = 83174405.000, x = 18240.000\n",
      "EPOCH 1: Cost = 53231621.000, x = 14592.000\n",
      "EPOCH 2: Cost = 34068239.240, x = 11673.600\n",
      "EPOCH 3: Cost = 21803674.914, x = 9338.880\n",
      "EPOCH 4: Cost = 13954353.745, x = 7471.104\n",
      "EPOCH 5: Cost = 8930788.197, x = 5976.883\n",
      "EPOCH 6: Cost = 5715706.246, x = 4781.507\n",
      "EPOCH 7: Cost = 3658053.797, x = 3825.205\n",
      "EPOCH 8: Cost = 2341156.230, x = 3060.164\n",
      "EPOCH 9: Cost = 1498341.787, x = 2448.131\n",
      "EPOCH 10: Cost = 958940.544, x = 1958.505\n",
      "EPOCH 11: Cost = 613723.748, x = 1566.804\n",
      "EPOCH 12: Cost = 392784.999, x = 1253.443\n",
      "EPOCH 13: Cost = 251384.199, x = 1002.755\n",
      "EPOCH 14: Cost = 160887.688, x = 802.204\n",
      "EPOCH 15: Cost = 102969.920, x = 641.763\n",
      "EPOCH 16: Cost = 65902.549, x = 513.410\n",
      "EPOCH 17: Cost = 42179.431, x = 410.728\n",
      "EPOCH 18: Cost = 26996.636, x = 328.583\n",
      "EPOCH 19: Cost = 17279.647, x = 262.866\n",
      "EPOCH 20: Cost = 11060.774, x = 210.293\n",
      "EPOCH 21: Cost = 7080.695, x = 168.234\n",
      "EPOCH 22: Cost = 4533.445, x = 134.587\n",
      "EPOCH 23: Cost = 2903.205, x = 107.670\n",
      "EPOCH 24: Cost = 1859.851, x = 86.136\n",
      "EPOCH 25: Cost = 1192.105, x = 68.909\n",
      "EPOCH 26: Cost = 764.747, x = 55.127\n",
      "EPOCH 27: Cost = 491.238, x = 44.102\n",
      "EPOCH 28: Cost = 316.192, x = 35.281\n",
      "EPOCH 29: Cost = 204.163, x = 28.225\n",
      "EPOCH 30: Cost = 132.464, x = 22.580\n",
      "EPOCH 31: Cost = 86.577, x = 18.064\n",
      "EPOCH 32: Cost = 57.209, x = 14.451\n",
      "EPOCH 33: Cost = 38.414, x = 11.561\n",
      "EPOCH 34: Cost = 26.385, x = 9.249\n",
      "EPOCH 35: Cost = 18.686, x = 7.399\n",
      "EPOCH 36: Cost = 13.759, x = 5.919\n",
      "EPOCH 37: Cost = 10.606, x = 4.735\n",
      "EPOCH 38: Cost = 8.588, x = 3.788\n",
      "EPOCH 39: Cost = 7.296, x = 3.031\n",
      "EPOCH 40: Cost = 6.470, x = 2.425\n",
      "EPOCH 41: Cost = 5.941, x = 1.940\n",
      "EPOCH 42: Cost = 5.602, x = 1.552\n",
      "EPOCH 43: Cost = 5.385, x = 1.241\n",
      "EPOCH 44: Cost = 5.247, x = 0.993\n",
      "EPOCH 45: Cost = 5.158, x = 0.794\n",
      "EPOCH 46: Cost = 5.101, x = 0.636\n",
      "EPOCH 47: Cost = 5.065, x = 0.508\n",
      "EPOCH 48: Cost = 5.041, x = 0.407\n",
      "EPOCH 49: Cost = 5.026, x = 0.325\n",
      "EPOCH 50: Cost = 5.017, x = 0.260\n",
      "EPOCH 51: Cost = 5.011, x = 0.208\n",
      "EPOCH 52: Cost = 5.007, x = 0.167\n",
      "EPOCH 53: Cost = 5.004, x = 0.133\n",
      "EPOCH 54: Cost = 5.003, x = 0.107\n",
      "EPOCH 55: Cost = 5.002, x = 0.085\n",
      "EPOCH 56: Cost = 5.001, x = 0.068\n",
      "EPOCH 57: Cost = 5.001, x = 0.055\n",
      "EPOCH 58: Cost = 5.000, x = 0.044\n",
      "EPOCH 59: Cost = 5.000, x = 0.035\n",
      "EPOCH 60: Cost = 5.000, x = 0.028\n",
      "EPOCH 61: Cost = 5.000, x = 0.022\n",
      "EPOCH 62: Cost = 5.000, x = 0.018\n",
      "EPOCH 63: Cost = 5.000, x = 0.014\n",
      "EPOCH 64: Cost = 5.000, x = 0.011\n",
      "EPOCH 65: Cost = 5.000, x = 0.009\n",
      "EPOCH 66: Cost = 5.000, x = 0.007\n",
      "EPOCH 67: Cost = 5.000, x = 0.006\n",
      "EPOCH 68: Cost = 5.000, x = 0.005\n",
      "EPOCH 69: Cost = 5.000, x = 0.004\n",
      "EPOCH 70: Cost = 5.000, x = 0.003\n",
      "EPOCH 71: Cost = 5.000, x = 0.002\n",
      "EPOCH 72: Cost = 5.000, x = 0.002\n",
      "EPOCH 73: Cost = 5.000, x = 0.002\n",
      "EPOCH 74: Cost = 5.000, x = 0.001\n",
      "EPOCH 75: Cost = 5.000, x = 0.001\n",
      "EPOCH 76: Cost = 5.000, x = 0.001\n",
      "EPOCH 77: Cost = 5.000, x = 0.001\n",
      "EPOCH 78: Cost = 5.000, x = 0.001\n",
      "EPOCH 79: Cost = 5.000, x = 0.000\n",
      "EPOCH 80: Cost = 5.000, x = 0.000\n",
      "EPOCH 81: Cost = 5.000, x = 0.000\n",
      "EPOCH 82: Cost = 5.000, x = 0.000\n",
      "EPOCH 83: Cost = 5.000, x = 0.000\n",
      "EPOCH 84: Cost = 5.000, x = 0.000\n",
      "EPOCH 85: Cost = 5.000, x = 0.000\n",
      "EPOCH 86: Cost = 5.000, x = 0.000\n",
      "EPOCH 87: Cost = 5.000, x = 0.000\n",
      "EPOCH 88: Cost = 5.000, x = 0.000\n",
      "EPOCH 89: Cost = 5.000, x = 0.000\n",
      "EPOCH 90: Cost = 5.000, x = 0.000\n",
      "EPOCH 91: Cost = 5.000, x = 0.000\n",
      "EPOCH 92: Cost = 5.000, x = 0.000\n",
      "EPOCH 93: Cost = 5.000, x = 0.000\n",
      "EPOCH 94: Cost = 5.000, x = 0.000\n",
      "EPOCH 95: Cost = 5.000, x = 0.000\n",
      "EPOCH 96: Cost = 5.000, x = 0.000\n",
      "EPOCH 97: Cost = 5.000, x = 0.000\n",
      "EPOCH 98: Cost = 5.000, x = 0.000\n",
      "EPOCH 99: Cost = 5.000, x = 0.000\n",
      "EPOCH 100: Cost = 5.000, x = 0.000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given the starting point of any `x` gradient descent\n",
    "should be able to find the minimum value of x for the\n",
    "cost function `f` defined below.\n",
    "\"\"\"\n",
    "import random\n",
    "#from gd import gradient_descent_update\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Quadratic function.\n",
    "\n",
    "    It's easy to see the minimum value of the function\n",
    "    is 5 when is x=0.\n",
    "    \"\"\"\n",
    "    return x**2 + 5\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"\n",
    "    Derivative of `f` with respect to `x`.\n",
    "    \"\"\"\n",
    "    return 2*x\n",
    "\n",
    "\n",
    "# Random number better 0 and 10,000. Feel free to set x whatever you like.\n",
    "x = random.randint(0, 10000)\n",
    "# TODO: Set the learning rate\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs+1):\n",
    "    cost = f(x)\n",
    "    gradx = df(x)\n",
    "    print(\"EPOCH {}: Cost = {:.3f}, x = {:.3f}\".format(i, cost, gradx))\n",
    "    x = gradient_descent_update(x, gradx, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tflearn1x]",
   "language": "python",
   "name": "conda-env-tflearn1x-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
